{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from lstm_classifier import LSTMClassifier\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from custom_dataset import CustomDataset\n",
    "import argparse\n",
    "from string import punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Funciones'''\n",
    "def get_lower_line_without_punctuation(line):\n",
    "    lower_line = line.lower()\n",
    "    return ''.join([c for c in lower_line if c not in punctuation])\n",
    "\n",
    "\n",
    "# Función que levanta el archivo data y lo transforma en una lista de (sentence, label)\n",
    "def get_data_splitted(data):\n",
    "    instances = []\n",
    "    labels = []\n",
    "    with open(data, 'r') as sentences:\n",
    "        for line in sentences:\n",
    "            processed_line = get_lower_line_without_punctuation(line)\n",
    "            instances.append(get_sentence_splitted(processed_line))\n",
    "            labels.append(get_label(line))\n",
    "    return instances, labels\n",
    "\n",
    "def get_label(line):\n",
    "    return line.split()[0].replace('__label__', '')\n",
    "\n",
    "def get_sentence_splitted(line):\n",
    "    line_split = line.split();\n",
    "    res = []\n",
    "    for val in range(1, len(line_split)):\n",
    "        res.append(line_split[val])\n",
    "    return res;\n",
    "\n",
    "### Función que toma un jsonl y agrega las palabras a mi mapa de word_to_integer\n",
    "def add_words_to_map(sentences, word_to_ix):\n",
    "    for sentence in sentences:\n",
    "        for word in sentence.split():\n",
    "            if word not in word_to_ix:\n",
    "                word_to_ix[word] = len(word_to_ix)\n",
    "    return word_to_ix\n",
    "\n",
    "\n",
    "\n",
    "### Función que recibe la lista de archivos txt, convierte cada uno en una lista de oraciones de Python y se encarga de llamar a add_words_to_map\n",
    "def create_map(txt_list):\n",
    "    word_to_ix = {}\n",
    "    for input_file in txt_list:\n",
    "        with open(input_file, 'r') as infile:\n",
    "            sentences = []\n",
    "            for line in infile:\n",
    "                processed_line = get_lower_line_without_punctuation(line)\n",
    "                sentences.append(processed_line)\n",
    "            word_to_ix = add_words_to_map(sentences, word_to_ix)\n",
    "    return word_to_ix\n",
    "\n",
    "\n",
    "# Función que crea un vector contando la cantidad de apariciones de las palabras en una oración.\n",
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = torch.zeros(len(word_to_ix)) # Vector de ceros\n",
    "    for word in sentence.split():\n",
    "        vec[word_to_ix[word]] += 1 # Por cada aparición de una palabra, le sumo uno\n",
    "    return vec.view(1, -1) # Vector de tamaño 1 x n, donde n es inferido por el tamaño de palabras\n",
    "\n",
    "\n",
    "# Función que wrappea la variable en un tensor. Básicamente, le pasas la lista de labels y tu label en particular, y te devuelve un tensor con el valor 0, 1 ó 2 adentro.\n",
    "def make_target(label, label_to_ix):\n",
    "    return label_to_ix[label]\n",
    "\n",
    "def get_label_by_item(item):\n",
    "    for label, value in label_to_ix.items():\n",
    "        if(value == item):\n",
    "            return label\n",
    "    return None\n",
    "\n",
    "\n",
    "def calculate_error_rate(predicted, label_batch):\n",
    "    counter = 0\n",
    "    ok = 0\n",
    "    for instance,label in zip(predicted, label_batch):\n",
    "        if(instance.item() == label.item()):\n",
    "            ok += 1\n",
    "        counter += 1\n",
    "\n",
    "    return ok / counter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def define_batch_size(batch_size, file):\n",
    "    size = 0\n",
    "    with open(file, 'r') as infile:\n",
    "        size = len(infile.readlines())\n",
    "    while True:\n",
    "        if(size % batch_size == 0):\n",
    "            return batch_size\n",
    "        batch_size = batch_size - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# TODO mover esto de aca\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = list(map(lambda w: to_ix[w], seq))\n",
    "    return idxs\n",
    "\n",
    "def get_result_label(result, label_to_ix):\n",
    "    for label, number in label_to_ix.items():    # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
    "        if result == number:\n",
    "            return \"__label__\"+label\n",
    "\n",
    "def get_tensor_data(data_inst, data_lab, word_to_ix, label_to_ix, use_labels=True):\n",
    "    instances = []\n",
    "    labels = []\n",
    "    for instance, label in zip(data_inst, data_lab):\n",
    "        instances.append(prepare_sequence(instance, word_to_ix))\n",
    "        if(use_labels):\n",
    "            labels.append(make_target(label, label_to_ix))\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    return instances, labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Entrenamiento'''\n",
    "# Usually you want to pass over the training data several times.\n",
    "# 100 is much bigger than on a real data set, but real datasets have more than\n",
    "# two instances.  Usually, somewhere between 5 and 30 epochs is reasonable (NOTA DE MARISCO: tarda algunos minutos cada vuelta).\n",
    "instances, labels = get_data_splitted(TRAIN_DATA)\n",
    "instances, labels = get_tensor_data(instances, labels, word_to_ix, label_to_ix)\n",
    "\n",
    "instances = instances[0:DATA]\n",
    "labels = labels[0:DATA]\n",
    "\n",
    "def get_max_length(x):\n",
    "    return len(max(x, key=len))\n",
    "\n",
    "def pad_sequence(seq):\n",
    "    def _pad(_it, _max_len):\n",
    "        return [0] * (_max_len - len(_it)) + _it\n",
    "    return [_pad(it, get_max_length(seq)) for it in seq]\n",
    "\n",
    "def custom_collate(batch):\n",
    "    transposed = zip(*batch)\n",
    "    lst = []\n",
    "    for samples in transposed:\n",
    "        if isinstance(samples[0], int):\n",
    "            lst.append(torch.LongTensor(samples))\n",
    "        elif isinstance(samples[0], float):\n",
    "            lst.append(torch.DoubleTensor(samples))\n",
    "        elif isinstance(samples[0], list):\n",
    "            lst.append(torch.LongTensor(pad_sequence(samples)))\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGING = False\n",
    "SHUFFLE = True # used to shuffle the trainset before each epoc\n",
    "DATA = 30000 # TODO: this datasize is hardcoded\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 50\n",
    "BATCH_SIZE = 50\n",
    "EPOCH_SIZE = 5\n",
    "CLIP = 5 # normalizing lstm vector values when backpropagating to avoid exploding gradients\n",
    "LEARNING_RATE = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_SENTENCES = \"./dev_sentences.txt\"\n",
    "TRAIN_SENTENCES = \"./train_sentences.txt\"\n",
    "TEST_SENTENCES = \"./test_sentences.txt\"\n",
    "\n",
    "DEV_DATA = \"./dev_data.txt\"\n",
    "TRAIN_DATA = \"./train_data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Creación del modelo '''\n",
    "### Defino la cantidad de palabras y la cantidad de labels\n",
    "label_to_ix = { \"neutral\": 0, \"contradiction\": 1, \"entailment\": 2 }\n",
    "word_to_ix = create_map([DEV_SENTENCES, TRAIN_SENTENCES, TEST_SENTENCES])\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = len(label_to_ix)\n",
    "\n",
    "# Creo mi modelo, defino la loss function, y la función de optimización\n",
    "model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, NUM_LABELS, BATCH_SIZE)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_data = CustomDataset(instances, labels)\n",
    "train_loader = DataLoader(dataset=tensor_data, batch_size=BATCH_SIZE, shuffle=SHUFFLE, collate_fn=custom_collate, drop_last=True) #TODO shuffle?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 STARTED\n",
      "0, loss: 0.7557068467140198 error: 0.58\n",
      "100, loss: 0.6739283204078674 error: 0.64\n",
      "200, loss: 0.6935069561004639 error: 0.66\n",
      "300, loss: 0.6125454306602478 error: 0.78\n",
      "400, loss: 0.8946799635887146 error: 0.54\n",
      "500, loss: 0.7875515222549438 error: 0.62\n",
      "EPOC ENDED: 0, avg_error: 0.6676000000000012\n",
      "EPOCH 1 STARTED\n",
      "0, loss: 0.574074387550354 error: 0.78\n",
      "100, loss: 0.6824177503585815 error: 0.7\n",
      "200, loss: 0.7725916504859924 error: 0.68\n",
      "300, loss: 0.797659158706665 error: 0.68\n",
      "400, loss: 0.7096593976020813 error: 0.74\n",
      "500, loss: 0.8395176529884338 error: 0.64\n",
      "EPOC ENDED: 1, avg_error: 0.6776666666666676\n",
      "EPOCH 2 STARTED\n",
      "0, loss: 0.7096903324127197 error: 0.7\n",
      "100, loss: 0.7102640271186829 error: 0.7\n",
      "200, loss: 0.8296381235122681 error: 0.58\n",
      "300, loss: 0.8198211193084717 error: 0.7\n",
      "400, loss: 0.5977815389633179 error: 0.78\n",
      "500, loss: 0.7157251238822937 error: 0.66\n",
      "EPOC ENDED: 2, avg_error: 0.6882666666666685\n",
      "EPOCH 3 STARTED\n",
      "0, loss: 0.7513614892959595 error: 0.64\n",
      "100, loss: 0.8304129838943481 error: 0.66\n",
      "200, loss: 0.7147794961929321 error: 0.7\n",
      "300, loss: 0.7642654180526733 error: 0.7\n",
      "400, loss: 0.6840789914131165 error: 0.66\n",
      "500, loss: 0.951794445514679 error: 0.56\n",
      "EPOC ENDED: 3, avg_error: 0.6971333333333345\n",
      "EPOCH 4 STARTED\n",
      "0, loss: 0.3821544647216797 error: 0.94\n",
      "100, loss: 0.7556579113006592 error: 0.62\n",
      "200, loss: 0.6976842284202576 error: 0.68\n",
      "300, loss: 0.7306006550788879 error: 0.66\n",
      "400, loss: 0.6317856907844543 error: 0.76\n",
      "500, loss: 0.9688704013824463 error: 0.62\n",
      "EPOC ENDED: 4, avg_error: 0.7051666666666674\n"
     ]
    }
   ],
   "source": [
    "error_rates_per_epoch = []\n",
    "for epoch in range(EPOCH_SIZE):\n",
    "    \n",
    "    print(\"EPOCH {} STARTED\".format(epoch))\n",
    "\n",
    "    error_rates_per_batch = []\n",
    "    i = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for instance_batch, label_batch in train_loader:\n",
    "        # Step 1. Pytorch accumulates gradients.  We need to clear them out\n",
    "        # before each instance\n",
    "        model.zero_grad()\n",
    "        model.hidden = model.init_hidden()\n",
    "        instance_batch = instance_batch.transpose(0,1)\n",
    "\n",
    "        # Step 2. Run our forward pass\n",
    "        log_probs = model(instance_batch)\n",
    "\n",
    "        # Step 3. Compute the loss, gradients, and update the parameters by calling\n",
    "        # optimizer.step()\n",
    "        loss = loss_function(log_probs, label_batch) # gets the a scalar value held in the loss\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), CLIP) # Gradient clip to avoid exploding gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss = loss.item()\n",
    "        _, predicted = torch.max(log_probs, 1)\n",
    "        \n",
    "        error_rate = calculate_error_rate(predicted, label_batch)\n",
    "        if not i % 100:\n",
    "            print(\"{}, loss: {} error: {}\".format(i, running_loss, error_rate))\n",
    "\n",
    "        error_rates_per_batch.append(error_rate)\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "    \n",
    "    epoch_average_error = sum(error_rates_per_batch) / len(error_rates_per_batch)\n",
    "    error_rates_per_epoch.append(epoch_error)\n",
    "    \n",
    "    print(\"EPOCH {} ENDED:  avg_error: {}\".format(epoch, epoch_average_error))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
