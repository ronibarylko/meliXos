{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "train_model_with_tensorboard.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronibarylko/meliXos/blob/master/src/train_model_with_tensorboard_use_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDVCnpwXvZIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "\n",
        "import time\n",
        "import argparse\n",
        "from string import punctuation\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK2uwPEavZIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Funciones'''\n",
        "# Función que levanta el archivo data y lo transforma en una lista de (sentence, label)\n",
        "def get_data_splitted(data):\n",
        "    instances = []\n",
        "    labels = []\n",
        "    with open(data, 'r') as sentences:\n",
        "        for line in sentences:\n",
        "            instances.append(get_sentence_splitted(line))\n",
        "            labels.append(get_label(line))\n",
        "    return instances, labels\n",
        "\n",
        "def get_label(line):\n",
        "    return line.split()[0].replace('__label__', '')\n",
        "\n",
        "def get_sentence_splitted(line):\n",
        "    line_split = line.split();\n",
        "    res = []\n",
        "    for val in range(1, len(line_split)):\n",
        "        res.append(line_split[val])\n",
        "    return res;\n",
        "\n",
        "### Función que toma un jsonl y agrega las palabras a mi mapa de word_to_integer\n",
        "def add_words_to_map(sentences, word_to_ix):\n",
        "    for sentence in sentences:\n",
        "        for word in sentence.split():\n",
        "            if word not in word_to_ix:\n",
        "                word_to_ix[word] = len(word_to_ix)\n",
        "    return word_to_ix\n",
        "\n",
        "### Función que recibe la lista de archivos txt, convierte cada uno en una lista de oraciones de Python y se encarga de llamar a add_words_to_map\n",
        "def create_map(txt_list):\n",
        "    word_to_ix = {}\n",
        "    for input_file in txt_list:\n",
        "        with open(input_file, 'r') as infile:\n",
        "            sentences = []\n",
        "            for line in infile:\n",
        "                sentences.append(line)\n",
        "            word_to_ix = add_words_to_map(sentences, word_to_ix)\n",
        "    return word_to_ix\n",
        "\n",
        "# Función que crea un vector contando la cantidad de apariciones de las palabras en una oración.\n",
        "def make_bow_vector(sentence, word_to_ix):\n",
        "    vec = torch.zeros(len(word_to_ix)) # Vector de ceros\n",
        "    for word in sentence.split():\n",
        "        vec[word_to_ix[word]] += 1 # Por cada aparición de una palabra, le sumo uno\n",
        "    return vec.view(1, -1) # Vector de tamaño 1 x n, donde n es inferido por el tamaño de palabras\n",
        "\n",
        "# Función que wrappea la variable en un tensor. Básicamente, le pasas la lista de labels y tu label en particular, y te devuelve un tensor con el valor 0, 1 ó 2 adentro.\n",
        "def make_target(label, label_to_ix):\n",
        "    return label_to_ix[label]\n",
        "\n",
        "def get_label_by_item(item):\n",
        "    for label, value in label_to_ix.items():\n",
        "        if(value == item):\n",
        "            return label\n",
        "    return None\n",
        "\n",
        "def calculate_error_rate(predicted, label_batch):\n",
        "    counter = 0\n",
        "    ok = 0\n",
        "    for instance,label in zip(predicted, label_batch):\n",
        "        if(instance.item() == label.item()):\n",
        "            ok += 1\n",
        "        counter += 1\n",
        "\n",
        "    return ok / counter\n",
        "\n",
        "def define_batch_size(batch_size, file):\n",
        "    size = 0\n",
        "    with open(file, 'r') as infile:\n",
        "        size = len(infile.readlines())\n",
        "    while True:\n",
        "        if(size % batch_size == 0):\n",
        "            return batch_size\n",
        "        batch_size = batch_size - 1\n",
        "\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = list(map(lambda w: to_ix[w], seq))\n",
        "    return idxs\n",
        "\n",
        "def get_result_label(result, label_to_ix):\n",
        "    for label, number in label_to_ix.items():    # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
        "        if result == number:\n",
        "            return \"__label__\"+label\n",
        "\n",
        "def get_tensor_data(data_inst, data_lab, word_to_ix, label_to_ix, use_labels=True):\n",
        "    instances = []\n",
        "    labels = []\n",
        "    for instance, label in zip(data_inst, data_lab):\n",
        "        instances.append(prepare_sequence(instance, word_to_ix))\n",
        "        if(use_labels):\n",
        "            labels.append(make_target(label, label_to_ix))\n",
        "        else:\n",
        "            labels.append(0)\n",
        "    return instances, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1cT7LVSvZIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Funciones utilizadas para el collate_fn de CustomDataset\n",
        "def get_max_length(x):\n",
        "    return len(max(x, key=len))\n",
        "\n",
        "def pad_sequence(seq):\n",
        "    def _pad(_it, _max_len):\n",
        "        return [0] * (_max_len - len(_it)) + _it\n",
        "    return [_pad(it, get_max_length(seq)) for it in seq]\n",
        "\n",
        "def custom_collate(batch):\n",
        "    transposed = zip(*batch)\n",
        "    lst = []\n",
        "    for samples in transposed:\n",
        "        if isinstance(samples[0], int):\n",
        "            lst.append(torch.LongTensor(samples))\n",
        "        elif isinstance(samples[0], float):\n",
        "            lst.append(torch.DoubleTensor(samples))\n",
        "        elif isinstance(samples[0], list):\n",
        "            lst.append(torch.LongTensor(pad_sequence(samples)))\n",
        "    return lst"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuaZM19ywLbi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, instances, labels):\n",
        "        self.instances = instances\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (self.instances[index], self.labels[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.instances)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw6RM9eiv0B9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, num_labels, batch_size, dropout=0.5, num_layers=2):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.dropout = dropout\n",
        "        self.num_layers = num_layers\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, bidirectional=True, dropout=dropout, num_layers=self.num_layers) #TODO aca le podemos meter layers, dropout, batch_first\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2label = nn.Linear(hidden_dim*2, num_labels)\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        # Before we've done anything, we dont have any hidden state.\n",
        "        # Refer to the Pytorch documentation to see exactly why they have this dimensionality.\n",
        "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
        "        return (autograd.Variable(torch.zeros(2*self.num_layers, self.batch_size, self.hidden_dim)),\n",
        "                autograd.Variable(torch.zeros(2*self.num_layers, self.batch_size, self.hidden_dim))) # La primera dimensión es 2 porque es bidireccional\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        embeds = embeds.view(len(sentence), self.batch_size, -1)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        label_space = self.hidden2label(lstm_out[-1])\n",
        "        label_scores = F.log_softmax(label_space, dim=1) # Softmax es la única con probabilidad. No tendría sentido algo como ReLu o Sigmoid si tenemos 3 posibilidades\n",
        "        return label_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A_uV0PLvZIy",
        "colab_type": "text"
      },
      "source": [
        "#### hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6ip6EkbvZIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOGGING = False\n",
        "SHUFFLE = True # used to shuffle the trainset before each epoc\n",
        "# DATA = 30000 # TODO: this datasize is hardcoded\n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 50\n",
        "BATCH_SIZE = 50\n",
        "EPOCH_SIZE = 15\n",
        "CLIP = 5 # normalizing lstm vector values when backpropagating to avoid exploding gradients\n",
        "LEARNING_RATE = 1\n",
        "DROPOUT=0.5\n",
        "NUM_LAYERS=2\n",
        "\n",
        "use_cuda = True # use GPU\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiJ5vixbvZI1",
        "colab_type": "text"
      },
      "source": [
        "#### file paths\n",
        "The following lines hardcode path to your specific google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54I_gfai0GFE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2da922b7-66cb-4d04-ff2b-d4221b1eeeb9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEqB50xk0Luy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_path= \"/content/drive/My Drive/eci_19_nlp/dataset/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5UMDjGuvZI2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEV_SENTENCES = dataset_path + \"dev_sentences.txt\"\n",
        "TRAIN_SENTENCES = dataset_path + \"/train_sentences.txt\"\n",
        "TEST_SENTENCES = dataset_path + \"test_sentences.txt\"\n",
        "\n",
        "DEV_DATA = dataset_path + \"dev_data.txt\"\n",
        "TRAIN_DATA = dataset_path + \"train_data.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCrcqdwnvZI4",
        "colab_type": "text"
      },
      "source": [
        "#### model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fec71NwvZI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' Creación del modelo '''\n",
        "### Defino la cantidad de palabras y la cantidad de labels\n",
        "label_to_ix = { \"neutral\": 0, \"contradiction\": 1, \"entailment\": 2 }\n",
        "word_to_ix = create_map([DEV_SENTENCES, TRAIN_SENTENCES, TEST_SENTENCES])\n",
        "VOCAB_SIZE = len(word_to_ix)\n",
        "NUM_LABELS = len(label_to_ix)\n",
        "\n",
        "# Creo mi modelo, defino la loss function, y la función de optimización\n",
        "model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, NUM_LABELS, BATCH_SIZE, DROPOUT, NUM_LAYERS)\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29m6eoMx91h3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f871dbfe-eb83-43e7-82df-381507efb231"
      },
      "source": [
        "if use_cuda and torch.cuda.is_available():\n",
        "  model = model.cuda()\n",
        "  print(\"Model loaded to GPU\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model loaded to GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5tTD62-C6JS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d0e1bca1-5797-421c-c5f6-d7132c60f40d"
      },
      "source": [
        "model"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMClassifier(\n",
              "  (word_embeddings): Embedding(53345, 100)\n",
              "  (lstm): LSTM(100, 50, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "  (hidden2label): Linear(in_features=100, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qq9sz94EvZI7",
        "colab_type": "text"
      },
      "source": [
        "#### training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpCbfBALvZI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Entrenamiento'''\n",
        "# Usually you want to pass over the training data several times.\n",
        "# 100 is much bigger than on a real data set, but real datasets have more than\n",
        "# two instances.  Usually, somewhere between 5 and 30 epochs is reasonable (NOTA DE MARISCO: tarda algunos minutos cada vuelta).\n",
        "instances, labels = get_data_splitted(TRAIN_DATA)\n",
        "instances, labels = get_tensor_data(instances, labels, word_to_ix, label_to_ix)\n",
        "\n",
        "# instances = instances[0:DATA]\n",
        "# labels = labels[0:DATA]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hxt8vFzvZI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tensor_data = CustomDataset(instances, labels)\n",
        "train_loader = DataLoader(dataset=tensor_data, batch_size=BATCH_SIZE, shuffle=SHUFFLE, collate_fn=custom_collate, drop_last=True)\n",
        "\n",
        "sentences, labels = next(iter(train_loader))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "08qYQez-vZJD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "outputId": "c5e213d3-22f4-45f9-8b60-f038627c93aa"
      },
      "source": [
        "# send model to tensorboard\n",
        "'''with SummaryWriter(\"./hello_tf_board/\") as writer:\n",
        "    writer.add_graph(model, sentences.transpose(0,1), True) # the transpose makes dims compatible (catofthecannals)\n",
        "'''\n",
        "error_rates_per_epoch = []\n",
        "for epoch in range(EPOCH_SIZE):\n",
        "    t = time.time()\n",
        "    print(\"EPOCH {} STARTED\".format(epoch))\n",
        "\n",
        "    error_rates_per_batch = []\n",
        "    i = 0\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    for instance_batch, label_batch in train_loader:\n",
        "      \n",
        "        # load data to GPU\n",
        "        if use_cuda and torch.cuda.is_available():\n",
        "            instance_batch, label_batch = instance_batch.cuda(), label_batch.cuda()\n",
        "      \n",
        "        # Step 1. Pytorch accumulates gradients.  We need to clear them out\n",
        "        # before each instance\n",
        "        model.zero_grad()\n",
        "        \n",
        "        if use_cuda and torch.cuda.is_available():\n",
        "            # model.hidden = tuple((elem.cuda() for elem in model.hidden))\n",
        "            model.hidden = (model.hidden[0].cuda(), model.hidden[1].cuda())\n",
        "\n",
        "        instance_batch = instance_batch.transpose(0,1)\n",
        "\n",
        "        # Step 2. Run our forward pass\n",
        "        log_probs = model(instance_batch)\n",
        "\n",
        "        # Step 3. Compute the loss, gradients, and update the parameters by calling\n",
        "        # optimizer.step()\n",
        "        loss = loss_function(log_probs, label_batch) # gets the a scalar value held in the loss\n",
        "        loss.backward(retain_graph=True)\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), CLIP) # Gradient clip to avoid exploding gradients\n",
        "        optimizer.step()\n",
        "        \n",
        "    # at the end of each epoc, log the loss and the acc of the last batch.\n",
        "    running_loss = loss.item()\n",
        "    _, predicted = torch.max(log_probs, 1)\n",
        "    error_rate = calculate_error_rate(predicted, label_batch)\n",
        "\n",
        "    with SummaryWriter(\"./hello_tf_board/\") as writer:\n",
        "        writer.add_scalar('accuracy', error_rate, epoch)\n",
        "        writer.add_scalar('loss function', running_loss, epoch)\n",
        "        \n",
        "    elapsed_time = time.time() - t\n",
        "    \n",
        "    print(\"EPOCH {} ENDED:  accuracy: {}, took {} s\".format(epoch, error_rate, elapsed_time))\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH 0 STARTED\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-647006025d75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# optimizer.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# gets the a scalar value held in the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Gradient clip to avoid exploding gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcx0xBxd1CsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}