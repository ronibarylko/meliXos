{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "train_model_with_tensorboard.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronibarylko/meliXos/blob/master/train_model_with_tensorboard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjNCQhaDHuVa",
        "colab_type": "text"
      },
      "source": [
        "# Predicting sentence motiv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I_wQu3CHvE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "\n",
        "import time\n",
        "import argparse\n",
        "from string import punctuation\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK2uwPEavZIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Funciones'''\n",
        "# Función que levanta el archivo data y lo transforma en una lista de (sentence, label)\n",
        "def get_data_splitted(data):\n",
        "    instances = []\n",
        "    labels = []\n",
        "    with open(data, 'r') as sentences:\n",
        "        for line in sentences:\n",
        "            instances.append(get_sentence_splitted(line))\n",
        "            labels.append(get_label(line))\n",
        "    return instances, labels\n",
        "\n",
        "def get_label(line):\n",
        "    return line.split()[0].replace('__label__', '')\n",
        "\n",
        "def get_sentence_splitted(line):\n",
        "    line_split = line.split();\n",
        "    res = []\n",
        "    for val in range(1, len(line_split)):\n",
        "        res.append(line_split[val])\n",
        "    return res;\n",
        "\n",
        "### Función que toma un jsonl y agrega las palabras a mi mapa de word_to_integer\n",
        "def add_words_to_map(sentences, word_to_ix):\n",
        "    for sentence in sentences:\n",
        "        for word in sentence.split():\n",
        "            if word not in word_to_ix:\n",
        "                word_to_ix[word] = len(word_to_ix)\n",
        "    return word_to_ix\n",
        "\n",
        "### Función que recibe la lista de archivos txt, convierte cada uno en una lista de oraciones de Python y se encarga de llamar a add_words_to_map\n",
        "def create_map(txt_list):\n",
        "    word_to_ix = {}\n",
        "    for input_file in txt_list:\n",
        "        with open(input_file, 'r') as infile:\n",
        "            sentences = []\n",
        "            for line in infile:\n",
        "                sentences.append(line)\n",
        "            word_to_ix = add_words_to_map(sentences, word_to_ix)\n",
        "    return word_to_ix\n",
        "\n",
        "# Función que crea un vector contando la cantidad de apariciones de las palabras en una oración.\n",
        "def make_bow_vector(sentence, word_to_ix):\n",
        "    vec = torch.zeros(len(word_to_ix)) # Vector de ceros\n",
        "    for word in sentence.split():\n",
        "        vec[word_to_ix[word]] += 1 # Por cada aparición de una palabra, le sumo uno\n",
        "    return vec.view(1, -1) # Vector de tamaño 1 x n, donde n es inferido por el tamaño de palabras\n",
        "\n",
        "# Función que wrappea la variable en un tensor. Básicamente, le pasas la lista de labels y tu label en particular, y te devuelve un tensor con el valor 0, 1 ó 2 adentro.\n",
        "def make_target(label, label_to_ix):\n",
        "    return label_to_ix[label]\n",
        "\n",
        "def get_label_by_item(item):\n",
        "    for label, value in label_to_ix.items():\n",
        "        if(value == item):\n",
        "            return label\n",
        "    return None\n",
        "\n",
        "def calculate_error_rate(predicted, label_batch):\n",
        "    counter = 0\n",
        "    ok = 0\n",
        "    for instance,label in zip(predicted, label_batch):\n",
        "        if(instance.item() == label.item()):\n",
        "            ok += 1\n",
        "        counter += 1\n",
        "\n",
        "    return ok / counter\n",
        "\n",
        "def define_batch_size(batch_size, file):\n",
        "    size = 0\n",
        "    with open(file, 'r') as infile:\n",
        "        size = len(infile.readlines())\n",
        "    while True:\n",
        "        if(size % batch_size == 0):\n",
        "            return batch_size\n",
        "        batch_size = batch_size - 1\n",
        "\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = list(map(lambda w: to_ix[w], seq))\n",
        "    return idxs\n",
        "\n",
        "def get_result_label(result, label_to_ix):\n",
        "    for label, number in label_to_ix.items():    # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
        "        if result == number:\n",
        "            return \"__label__\"+label\n",
        "\n",
        "def get_tensor_data(data_inst, data_lab, word_to_ix, label_to_ix, use_labels=True):\n",
        "    instances = []\n",
        "    labels = []\n",
        "    for instance, label in zip(data_inst, data_lab):\n",
        "        instances.append(prepare_sequence(instance, word_to_ix))\n",
        "        if(use_labels):\n",
        "            labels.append(make_target(label, label_to_ix))\n",
        "        else:\n",
        "            labels.append(0)\n",
        "    return instances, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1cT7LVSvZIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Funciones utilizadas para el collate_fn de CustomDataset\n",
        "def get_max_length(x):\n",
        "    return len(max(x, key=len))\n",
        "\n",
        "def pad_sequence(seq):\n",
        "    def _pad(_it, _max_len):\n",
        "        return [0] * (_max_len - len(_it)) + _it\n",
        "    return [_pad(it, get_max_length(seq)) for it in seq]\n",
        "\n",
        "def custom_collate(batch):\n",
        "    transposed = zip(*batch)\n",
        "    lst = []\n",
        "    for samples in transposed:\n",
        "        if isinstance(samples[0], int):\n",
        "            lst.append(torch.LongTensor(samples))\n",
        "        elif isinstance(samples[0], float):\n",
        "            lst.append(torch.DoubleTensor(samples))\n",
        "        elif isinstance(samples[0], list):\n",
        "            lst.append(torch.LongTensor(pad_sequence(samples)))\n",
        "    return lst"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuaZM19ywLbi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, instances, labels):\n",
        "        self.instances = instances\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (self.instances[index], self.labels[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.instances)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiJ5vixbvZI1",
        "colab_type": "text"
      },
      "source": [
        "#### mount google drive\n",
        "pay atention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54I_gfai0GFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdYhPI0sHGxw",
        "colab_type": "text"
      },
      "source": [
        "**dataset**\n",
        "\n",
        "Copy the following files to your google drive in a *specific path*: \n",
        "\n",
        "https://drive.google.com/drive/folders/1YZKBfUIrIsKvfV_alUHzECJ1QMKj_71f?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEqB50xk0Luy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this comment is hardcoded for my specific path in drive\n",
        "dataset_path= \"/content/drive/My Drive/eci_19_nlp/dataset/\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5UMDjGuvZI2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEV_SENTENCES = dataset_path + \"dev_sentences.txt\"\n",
        "TRAIN_SENTENCES = dataset_path + \"train_sentences.txt\"\n",
        "TEST_SENTENCES = dataset_path + \"test_sentences.txt\"\n",
        "\n",
        "DEV_DATA = dataset_path + \"dev_data.txt\"\n",
        "TRAIN_DATA = dataset_path + \"train_data.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCrcqdwnvZI4",
        "colab_type": "text"
      },
      "source": [
        "#### model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvMiCn4OLmnC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, num_labels, batch_size, dropout=0.5, num_layers=2):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.dropout = dropout\n",
        "        self.num_layers = num_layers\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, bidirectional=True, dropout=dropout, num_layers=self.num_layers) #TODO aca le podemos meter layers, dropout, batch_first\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2label = nn.Linear(hidden_dim*2, num_labels)\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        # Before we've done anything, we dont have any hidden state.\n",
        "        # Refer to the Pytorch documentation to see exactly why they have this dimensionality.\n",
        "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
        "        return (autograd.Variable(torch.zeros(2*self.num_layers, self.batch_size, self.hidden_dim)),\n",
        "                autograd.Variable(torch.zeros(2*self.num_layers, self.batch_size, self.hidden_dim))) # La primera dimensión es 2 porque es bidireccional\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        embeds = embeds.view(len(sentence), self.batch_size, -1)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        label_space = self.hidden2label(lstm_out[-1])\n",
        "        label_scores = F.log_softmax(label_space, dim=1) # Softmax es la única con probabilidad. No tendría sentido algo como ReLu o Sigmoid si tenemos 3 posibilidades\n",
        "        return label_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7s3U7BWLc2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' Creación del modelo '''\n",
        "### Defino la cantidad de palabras y la cantidad de labels\n",
        "label_to_ix = { \"neutral\": 0, \"contradiction\": 1, \"entailment\": 2 }\n",
        "word_to_ix = create_map([DEV_SENTENCES, TRAIN_SENTENCES, TEST_SENTENCES])\n",
        "VOCAB_SIZE = len(word_to_ix)\n",
        "NUM_LABELS = len(label_to_ix)\n",
        "# Creo mi modelo, defino la loss function, y la función de optimización\n",
        "model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, NUM_LABELS, BATCH_SIZE, DROPOUT, NUM_LAYERS)\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKInVlXIJSdH",
        "colab_type": "text"
      },
      "source": [
        "#### GPU\n",
        "To enable it, go to \"edit\" and then \"Notebook settings\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29m6eoMx91h3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if use_cuda and torch.cuda.is_available():\n",
        "  model = model.cuda()\n",
        "  print(\"Model loaded to GPU\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5tTD62-C6JS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMAj05LpK-PZ",
        "colab_type": "text"
      },
      "source": [
        "#### hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWYlPmWCK89D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOGGING = False\n",
        "SHUFFLE = True # used to shuffle the trainset before each epoc\n",
        "# DATA = 30000 # TODO: this datasize is hardcoded\n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 50\n",
        "BATCH_SIZE = 50\n",
        "EPOCH_SIZE = 15\n",
        "CLIP = 5 # normalizing lstm vector values when backpropagating to avoid exploding gradients\n",
        "LEARNING_RATE = 1\n",
        "DROPOUT=0.5\n",
        "NUM_LAYERS=2\n",
        "\n",
        "use_cuda = True # use GPU\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qq9sz94EvZI7",
        "colab_type": "text"
      },
      "source": [
        "#### training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpCbfBALvZI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Entrenamiento'''\n",
        "# Usually you want to pass over the training data several times.\n",
        "# 100 is much bigger than on a real data set, but real datasets have more than\n",
        "# two instances.  Usually, somewhere between 5 and 30 epochs is reasonable (NOTA DE MARISCO: tarda algunos minutos cada vuelta).\n",
        "instances, labels = get_data_splitted(TRAIN_DATA)\n",
        "instances, labels = get_tensor_data(instances, labels, word_to_ix, label_to_ix)\n",
        "\n",
        "# instances = instances[0:DATA]\n",
        "# labels = labels[0:DATA]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hxt8vFzvZI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tensor_data = CustomDataset(instances, labels)\n",
        "train_loader = DataLoader(dataset=tensor_data, batch_size=BATCH_SIZE, shuffle=SHUFFLE, collate_fn=custom_collate, drop_last=True)\n",
        "\n",
        "sentences, labels = next(iter(train_loader))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "08qYQez-vZJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# send model to tensorboard\n",
        "'''with SummaryWriter(\"./hello_tf_board/\") as writer:\n",
        "    writer.add_graph(model, sentences.transpose(0,1), True) # the transpose makes dims compatible (catofthecannals)\n",
        "'''\n",
        "error_rates_per_epoch = []\n",
        "for epoch in range(EPOCH_SIZE):\n",
        "    t = time.time()\n",
        "    print(\"EPOCH {} STARTED\".format(epoch))\n",
        "\n",
        "    error_rates_per_batch = []\n",
        "    i = 0\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    for instance_batch, label_batch in train_loader:\n",
        "      \n",
        "        # load data to GPU\n",
        "        if use_cuda and torch.cuda.is_available():\n",
        "            instance_batch, label_batch = instance_batch.cuda(), label_batch.cuda()\n",
        "      \n",
        "        model.zero_grad()\n",
        "      \n",
        "        # Step 1. Pytorch accumulates gradients.  We need to clear them out\n",
        "        # before each instance        \n",
        "        if use_cuda and torch.cuda.is_available():\n",
        "            model.hidden = model.init_hidden()\n",
        "            model.hidden = (model.hidden[0].cuda(), model.hidden[1].cuda())\n",
        "\n",
        "        \n",
        "            \n",
        "        instance_batch = instance_batch.transpose(0,1)\n",
        "\n",
        "        # Step 2. Run our forward pass\n",
        "        log_probs = model(instance_batch)\n",
        "\n",
        "        # Step 3. Compute the loss, gradients, and update the parameters by calling\n",
        "        # optimizer.step()\n",
        "        loss = loss_function(log_probs, label_batch) # gets the a scalar value held in the loss\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), CLIP) # Gradient clip to avoid exploding gradients\n",
        "        optimizer.step()\n",
        "        \n",
        "    # at the end of each epoc, log the loss and the acc of the last batch.\n",
        "    running_loss = loss.item()\n",
        "    _, predicted = torch.max(log_probs, 1)\n",
        "    error_rate = calculate_error_rate(predicted, label_batch)\n",
        "\n",
        "    with SummaryWriter(\"./hello_tf_board/\") as writer:\n",
        "        writer.add_scalar('accuracy', error_rate, epoch)\n",
        "        writer.add_scalar('loss function', running_loss, epoch)\n",
        "        \n",
        "    elapsed_time = time.time() - t\n",
        "    \n",
        "    print(\"EPOCH {} ENDED:  accuracy: {}, took {} s\".format(epoch, error_rate, elapsed_time))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcx0xBxd1CsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}